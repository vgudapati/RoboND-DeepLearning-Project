# RoboND Deep Learning Project

[//]: # (Image References)

[semantic_segmentation_definition]: imgs/img_semantic_segmentation.png
[img_fcn_paper_1]: imgs/img_fcn_paper_1.png

[img_cs231n_cnn_basics_1]: imgs/img_cs231n_cnn_basics_1.png
[img_cs231n_cnn_basics_2]: imgs/img_cs231n_basics_output_volume.png
[img_1x1_convolutions]: imgs/img_1x1_convolutions.png
[img_cs231n_unpooling]: imgs/img_unpooling.png
[img_cs231n_transpose_convolutions]: imgs/img_transpose_convolutions.png
[img_cs231n_max_pooling]: imgs/img_max_pooling.png
[img_fcn_architecture_vgg]: imgs/img_fcn_model_vgg.png
[img_model_summary_vgg]: imgs/model.png

[target]: imgs/target.png
[no_target]: imgs/no_target.png
[distant_target]: imgs/distant_target.png
[results]: imgs/results.png

## Introduction

The objective of this project is to implement a "follow-me" application in simulated environment. The main task in the project 
is to locate and follow a moving target. The images are coming from the camera on the drone and each image we received is 
classified using a fully convolutional neural network by leveraging semantic segmentation.  

[image_0]: ./docs/misc/sim_screenshot.png
![alt text][image_0] 

## Outline

The following outline is followed for the project.

1.  [Semantic segmentation](#semantic_segmentation)
2.  [Fully convolutional networks](#fully_convolutional_networks)
3.  [Techniques and concepts](#techniques_and_concepts)
4.  [Set up the environment](#set_up_the_environment)
5.  [Data Collection and processing](#data_collection_and_processing)
6.  [Network architecture and implementation](#network_architecture_and_implementation)
7.  [Hyper parameters](#hyper_parameters)
8.  [Limitations and challenges](#limitations_and_challenges)
9.  [Model Weights](#model_weights)
10. [Accuracy and Performance](#accuracy_and_performance)
11. [Future work](#future_work)

## **1. Semantic segmentation** <a id='semantic_segmentation'></a>

Semantic segmentation describes the process of associating each pixel of an image with a class label (such as flower, person, 
road, sky, ocean, or car). The output of semantic segmentation is an image with pixels values representing the one-hot encoded 
class assigned.

![SEMANTIC SEGMENTATION DEFINITION][semantic_segmentation_definition]

## **2. Fully Convolutional Networks** <a id='fully_convolutional_networks'></a>

A Fully Convolutional neural network (FCN) is a normal CNN, where the last fully connected layer is substituted by another convolution layer with a large "receptive field". The general architecture is described in [**this**](https://people.eecs.berkeley.edu/%7Ejonlong/long_shelhamer_fcn.pdf) paper. The following image ( from [1] ) shows the general architecture described in the paper.

![FCN from paper 1][img_fcn_paper_1]

- In a normal convolutional neural network, we have a set of convolutional layers followed by fully connected layer. The initial set of convolutional layers makes the encoder part of the FCN. In FCN, the fully connected layer becomes a convolutional layer. 
- From the convoluted fully connected layer, we upsample the layers and this part forms the decoder network of the FCN.
- Some of the layers of encoder and decoder networks are connected to make what are known as skip connections for end-to-end, joint learning of semantics and location.
- The skip layers are refined by using different strides.

The advantages of FCN include the ability to 
  - train full image at a time without sampling
  - reshape network to take any size input

## **3. Techniques and concepts** <a id='techniques_and_concepts'></a>

### _**3.1. Convolutional layers**_

Convolutional layers operate by convolving filters over an input volume. The convolution operator is a dot-product of these filters over a portion of the input volume, and then sliding this **receptive field** over the entire input volume to get the resultant output volume of the convolution operation.

The following image illustrates the process.
(Stanford's cs231n [**lecture**](http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf))

![CNN LAYER OVERVIEW 1][img_cs231n_cnn_basics_1]

The resulting output volume is generated by applying all the filters in the convolutional layer and stacking the resulting activation maps into a single output volume.

The following image illustrates the process.
(Stanford's cs231n [**lecture**](http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf))

![CNN LAYER OVERVIEW 2][img_cs231n_cnn_basics_2]

### _**3.2. 1x1 Convolutions**_

1x1 convolutions are convolution layers with filters of kernel 1x1 and strides of 1. The key aspects that make the use 1x1 convolutions a good resource to use in network architectures :

*   It's the same as a fully connected layer, but with the spatial dimensions in the output volume. 
*   They provide a way to add non-linearity without adding many parameters. 
*   They help increase or decrease the dimensionality of our working volumes by just setting the required number of filters.

![1x1 CONVOLUTION][img_1x1_convolutions]

## **4. Set up the environment** <a id='set_up_the_environment'></a>

The instructions for the local setup are followed from the [**lecture notes**](https://classroom.udacity.com/nanodegrees/nd209/parts/c199593e-1e9a-4830-8e29-2c86f70f489e/modules/cac27683-d5f4-40b4-82ce-d708de8f5373/lessons/197a058e-44f6-47df-8229-0ce633e0a2d0/concepts/06dde5a5-a7a2-4636-940d-e844b36ddd27)


## **5. Data Collection and processing** <a id='data_collection_and_processing'></a>

Data is collected using the tips given in the [**lecture**](https://classroom.udacity.com/nanodegrees/nd209/parts/c199593e-1e9a-4830-8e29-2c86f70f489e/modules/cac27683-d5f4-40b4-82ce-d708de8f5373/lessons/197a058e-44f6-47df-8229-0ce633e0a2d0/concepts/764666fe-b285-4b19-ac2f-02217084455a)

I tried 3 different sets of data. But unfortunately, the results were not good for the self collected data and as it was quite time consuming considering where i'm with respect to deadlines, i pivoted to the data provided as part of the project.

And then i tried multiple network architectures to get the necessary accuracy needed for the project.

## **6. Network architecture and implementation** <a id='network_architecture_and_implementation'></a>

Since I was sticking to the data provided for the project due to time constraints, I had to make sure the network architecture needed to perform.

As in the notebook, i have tried 4 different architectures. The fcn_model_var1, fcn_model_var2 and fcn_model_var3 are purely based on the semantic segmentation lab in the letcures.

These models didn't give required accuracy inspite of multiple trial and various hyper parameter tuning.

Finally the i switched fcn_model_vgg which gave the necessary accuracy needed for the project.

The following are the network parameters and the network architecture.

![FCN_VGG ARCHITECTURE][img_fcn_architecture_vgg]

![FCN_VGG_MODEL][img_model_summary_vgg]

The architecture parameters followed from standard vgg 16 net parameters.

## **7. Hyper parameters** <a id='hyper_parameters'></a>

The hyperparameters that were used finally are:

learning_rate = 0.005
batch_size = 64
num_epochs = 20
steps_per_epoch = 150
validation_steps = 40

Before finalizing these parameters, all the parameters were varied along with different network architectures. Until swithced to fcn_model_vgg, the network never reached the required final score of 40%.

The details of the experiments conducted can be seen in model_training_experiments.ipynb notebook.

The final model is presented in model_training.ipynb.

## **8. Limitations and challenges** <a id='limitations_and_challenges'></a>

Due to the size of the vgg network, we may need higher resources. The data collection problems i faced, have forced me to choose more sophisticated network. But i'll definitely solve and spend more time on the data collection to play around with it.

## **9. Model Weights** <a id='model_weights'></a>

The models weights are saved to data/weights/model_weights_lr_005_bs_64_e20_spe_150_vs_40_run_3.h5 file so that we can use them to test the simulator to follow the moving target.

## **10. Accuracy and Performance** <a id='accuracy_and_performance'></a>

The image with target. 

![ACC_1][target]

The image with no target after training.

![ACC_2][no_target]

The image with distant target.

![ACC_3][distant_target]

The following are the results for IOU scores.

![ACC_4][results]

The video following the target can be found in videos directory.

## **11. Future work** <a id='future_work'></a>

Semantic segmentation is an exciting fields with applicability in many fields. The current work is very limited in its scope and to master a lot of use cases have to be executed. While i'm really glad to have the opportunity, the scope for future work is limitless.

Some of the ideas for future work are:
  - we can try more architectures such as unet, various encoding networks. The core paper for FCN's say the performance for FCN-8s is better than FCN-16s or FCN-32s
  - Other general tuning techniques such as regularization can be used to improve performance. 
  - Play around with skip connections.
